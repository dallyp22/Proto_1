{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ag IQ Feature Engineering\n",
        "\n",
        "This notebook creates engineered features from the cleaned dataset to prepare for model training.\n",
        "\n",
        "**Goal**: Transform cleaned data into model-ready features:\n",
        "- Equipment features (age, utilization, production status)\n",
        "- Temporal features (seasonality, cyclical patterns)\n",
        "- Macro features (normalized economic indicators)\n",
        "- Density features (volume patterns, rare category handling)\n",
        "\n",
        "**Expected Output**: ~20-30 engineered features ready for LightGBM training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: Setup\n",
        "# =============================================================================\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "print(\"Setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2: Load Processed Data\n",
        "# =============================================================================\n",
        "print(\"Loading processed data...\")\n",
        "training_df = pd.read_parquet('../data/processed/training_data.parquet')\n",
        "\n",
        "print(f\"Loaded: {len(training_df):,} records\")\n",
        "print(f\"Columns: {len(training_df.columns)}\")\n",
        "print(f\"Date range: {training_df['sold_date'].min().date()} to {training_df['sold_date'].max().date()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: Initialize Feature Pipeline\n",
        "# =============================================================================\n",
        "from src.features.pipeline import FeaturePipeline, FeatureConfig\n",
        "\n",
        "# Create feature pipeline with custom config\n",
        "config = FeatureConfig(\n",
        "    hours_per_year_cap=2000,\n",
        "    equipment_age_cap=50,\n",
        "    min_category_frequency=20\n",
        ")\n",
        "\n",
        "pipeline = FeaturePipeline(config=config)\n",
        "\n",
        "print(\"Feature pipeline initialized\")\n",
        "print(f\"Config: {pipeline.config}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 4: Fit and Transform Features\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Fit and transform in one step\n",
        "featured_df = pipeline.fit_transform(training_df)\n",
        "\n",
        "print(f\"\\nFeature engineering complete!\")\n",
        "print(f\"Input columns: {len(training_df.columns)}\")\n",
        "print(f\"Output columns: {len(featured_df.columns)}\")\n",
        "print(f\"New features created: {len(featured_df.columns) - len(training_df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 5: Inspect Engineered Features\n",
        "# =============================================================================\n",
        "cat_features, num_features = pipeline.get_feature_columns()\n",
        "\n",
        "print(\"\\n--- CATEGORICAL FEATURES ---\")\n",
        "for i, feat in enumerate(cat_features, 1):\n",
        "    if feat in featured_df.columns:\n",
        "        unique = featured_df[feat].nunique()\n",
        "        print(f\"  {i}. {feat:25} {unique} unique values\")\n",
        "\n",
        "print(f\"\\n--- NUMERIC FEATURES ---\")\n",
        "for i, feat in enumerate(num_features, 1):\n",
        "    if feat in featured_df.columns:\n",
        "        non_null = featured_df[feat].notna().sum()\n",
        "        pct = non_null / len(featured_df) * 100\n",
        "        print(f\"  {i:2}. {feat:25} {pct:5.1f}% coverage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Sample Feature Values\n",
        "# =============================================================================\n",
        "print(\"\\n--- SAMPLE EQUIPMENT FEATURES ---\")\n",
        "equipment_cols = ['equipment_age', 'hours_per_year', 'utilization_bucket', \n",
        "                  'is_current_production', 'years_since_discontinued']\n",
        "display_eq = [c for c in equipment_cols if c in featured_df.columns]\n",
        "print(featured_df[display_eq].head(10))\n",
        "\n",
        "print(\"\\n--- SAMPLE TEMPORAL FEATURES ---\")\n",
        "temporal_cols = ['sale_month', 'sale_quarter', 'month_sin', 'month_cos',\n",
        "                 'is_planting_season', 'is_harvest_season']\n",
        "display_temp = [c for c in temporal_cols if c in featured_df.columns]\n",
        "print(featured_df[display_temp].head(10))\n",
        "\n",
        "print(\"\\n--- SAMPLE MACRO FEATURES ---\")\n",
        "macro_cols = ['barometer_norm', 'sentiment_spread', 'investment_confidence', \n",
        "              'diesel_relative', 'el_nino_phase']\n",
        "display_macro = [c for c in macro_cols if c in featured_df.columns]\n",
        "print(featured_df[display_macro].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 7: Visualize Key Features\n",
        "# =============================================================================\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Equipment age\n",
        "if 'equipment_age' in featured_df.columns:\n",
        "    axes[0, 0].hist(featured_df['equipment_age'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[0, 0].set_xlabel('Equipment Age (years)')\n",
        "    axes[0, 0].set_ylabel('Count')\n",
        "    axes[0, 0].set_title('Equipment Age Distribution')\n",
        "    axes[0, 0].axvline(featured_df['equipment_age'].median(), color='red', linestyle='--',\n",
        "                       label=f\"Median: {featured_df['equipment_age'].median():.1f}\")\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "# Hours per year\n",
        "if 'hours_per_year' in featured_df.columns:\n",
        "    axes[0, 1].hist(featured_df['hours_per_year'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[0, 1].set_xlabel('Hours per Year')\n",
        "    axes[0, 1].set_ylabel('Count')\n",
        "    axes[0, 1].set_title('Utilization Rate Distribution')\n",
        "    axes[0, 1].axvline(featured_df['hours_per_year'].median(), color='red', linestyle='--',\n",
        "                       label=f\"Median: {featured_df['hours_per_year'].median():.0f}\")\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "# Utilization bucket\n",
        "if 'utilization_bucket' in featured_df.columns:\n",
        "    util_counts = featured_df['utilization_bucket'].value_counts()\n",
        "    axes[0, 2].bar(range(len(util_counts)), util_counts.values, alpha=0.7)\n",
        "    axes[0, 2].set_xticks(range(len(util_counts)))\n",
        "    axes[0, 2].set_xticklabels(util_counts.index, rotation=45)\n",
        "    axes[0, 2].set_ylabel('Count')\n",
        "    axes[0, 2].set_title('Utilization Buckets')\n",
        "\n",
        "# Seasonal patterns\n",
        "if 'sale_month' in featured_df.columns:\n",
        "    monthly_avg_price = featured_df.groupby('sale_month')['price'].median()\n",
        "    axes[1, 0].plot(monthly_avg_price.index, monthly_avg_price.values, marker='o')\n",
        "    axes[1, 0].set_xlabel('Month')\n",
        "    axes[1, 0].set_ylabel('Median Price ($)')\n",
        "    axes[1, 0].set_title('Price Seasonality')\n",
        "    axes[1, 0].set_xticks(range(1, 13))\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Barometer vs Price\n",
        "if 'barometer_norm' in featured_df.columns:\n",
        "    sample = featured_df.sample(min(5000, len(featured_df)))\n",
        "    axes[1, 1].scatter(sample['barometer_norm'], sample['price'], alpha=0.3, s=1)\n",
        "    axes[1, 1].set_xlabel('Normalized Barometer')\n",
        "    axes[1, 1].set_ylabel('Price ($)')\n",
        "    axes[1, 1].set_title('Economic Sentiment vs Price')\n",
        "\n",
        "# Make volume distribution\n",
        "if 'log_make_volume' in featured_df.columns:\n",
        "    axes[1, 2].hist(featured_df['log_make_volume'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[1, 2].set_xlabel('Log(Make Volume)')\n",
        "    axes[1, 2].set_ylabel('Count')\n",
        "    axes[1, 2].set_title('Make Volume Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 8: Feature Statistics\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get all numeric features that exist\n",
        "available_num_features = [f for f in num_features if f in featured_df.columns]\n",
        "\n",
        "if available_num_features:\n",
        "    print(f\"\\nNumeric features summary:\")\n",
        "    print(featured_df[available_num_features].describe().round(2))\n",
        "\n",
        "print(f\"\\n--- Missing Value Summary ---\")\n",
        "missing_summary = featured_df[available_num_features].isnull().sum()\n",
        "missing_pct = (missing_summary / len(featured_df) * 100).round(1)\n",
        "missing_df = pd.DataFrame({\n",
        "    'missing_count': missing_summary,\n",
        "    'missing_pct': missing_pct\n",
        "})\n",
        "missing_df = missing_df[missing_df['missing_count'] > 0].sort_values('missing_count', ascending=False)\n",
        "if len(missing_df) > 0:\n",
        "    print(missing_df)\n",
        "else:\n",
        "    print(\"No missing values in numeric features!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 9: Save Feature-Engineered Dataset and Pipeline\n",
        "# =============================================================================\n",
        "# Save featured dataset\n",
        "features_path = Path('../data/features/training_features.parquet')\n",
        "features_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Saving feature-engineered dataset...\")\n",
        "featured_df.to_parquet(features_path, index=False)\n",
        "\n",
        "file_size_mb = features_path.stat().st_size / (1024 * 1024)\n",
        "print(f\"✓ Saved to: {features_path}\")\n",
        "print(f\"  File size: {file_size_mb:.1f} MB\")\n",
        "print(f\"  Records: {len(featured_df):,}\")\n",
        "print(f\"  Total columns: {len(featured_df.columns)}\")\n",
        "\n",
        "# Save fitted pipeline\n",
        "pipeline_path = Path('../models/feature_pipeline.joblib')\n",
        "pipeline_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "pipeline.save(str(pipeline_path))\n",
        "\n",
        "print(f\"\\n✓ Pipeline saved to: {pipeline_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nEngineered Features Summary:\")\n",
        "print(f\"  - Categorical features: {len([f for f in cat_features if f in featured_df.columns])}\")\n",
        "print(f\"  - Numeric features: {len([f for f in num_features if f in featured_df.columns])}\")\n",
        "print(f\"  - Total feature columns: {len(cat_features) + len(num_features)}\")\n",
        "print(f\"\\nNext step: Model training (notebook 04)\")\n",
        "print(\"  - Train LightGBM FMV model\")\n",
        "print(\"  - Time-based train/val/test splits\")\n",
        "print(\"  - Target: <10% MAPE\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
