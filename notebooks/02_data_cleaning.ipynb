{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ag IQ Data Cleaning & Merging\n",
        "\n",
        "This notebook cleans the auction data and merges it with macro economic indicators to create the training dataset.\n",
        "\n",
        "**Goal**: Create a clean, merged dataset ready for feature engineering with:\n",
        "- Valid price, date, and equipment information\n",
        "- Macro indicators joined by month\n",
        "- 7-year training window (2018-2025)\n",
        "- ~600K+ usable records\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: Setup\n",
        "# =============================================================================\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "print(\"Setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2: Load Raw Data\n",
        "# =============================================================================\n",
        "from src.data.loaders import load_all_data\n",
        "\n",
        "print(\"Loading raw data...\")\n",
        "data = load_all_data('../data/raw')\n",
        "\n",
        "auctions = data['auctions']\n",
        "barometer = data['barometer']\n",
        "diesel = data['diesel']\n",
        "el_nino = data['el_nino']\n",
        "futures = data['futures']\n",
        "makes = data['makes']\n",
        "auctioneers = data['auctioneers']\n",
        "\n",
        "print(f\"\\nRaw auction records: {len(auctions):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: Create Training Dataset\n",
        "# =============================================================================\n",
        "from src.data.processors import create_training_dataset\n",
        "\n",
        "# Create cleaned and merged dataset\n",
        "# Using last 7 years of data (2018-2025) for training\n",
        "training_df = create_training_dataset(\n",
        "    auctions=auctions,\n",
        "    makes=makes,\n",
        "    barometer=barometer,\n",
        "    diesel=diesel,\n",
        "    el_nino=el_nino,\n",
        "    futures=futures,\n",
        "    min_date='2018-01-01',\n",
        "    max_date=None,  # Use all data up to present\n",
        "    min_price=1000,\n",
        "    max_price=2_000_000\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal training dataset: {len(training_df):,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 4: Data Quality Check\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA QUALITY CHECK\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTotal records: {len(training_df):,}\")\n",
        "print(f\"Date range: {training_df['sold_date'].min().date()} to {training_df['sold_date'].max().date()}\")\n",
        "print(f\"Price range: ${training_df['price'].min():,.0f} to ${training_df['price'].max():,.0f}\")\n",
        "\n",
        "print(\"\\n--- Core Fields Coverage ---\")\n",
        "core_fields = ['price', 'sold_date', 'year', 'hours', 'make_key', 'region']\n",
        "for field in core_fields:\n",
        "    if field in training_df.columns:\n",
        "        coverage = training_df[field].notna().mean() * 100\n",
        "        print(f\"  {field:15} {coverage:5.1f}%\")\n",
        "\n",
        "print(\"\\n--- Macro Indicators Coverage ---\")\n",
        "macro_fields = ['barometer', 'diesel_price', 'el_nino_phase']\n",
        "for field in macro_fields:\n",
        "    if field in training_df.columns:\n",
        "        coverage = training_df[field].notna().mean() * 100\n",
        "        print(f\"  {field:15} {coverage:5.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 5: Temporal Distribution\n",
        "# =============================================================================\n",
        "print(\"\\n--- Temporal Distribution ---\")\n",
        "\n",
        "# Monthly counts\n",
        "monthly_counts = training_df.groupby(training_df['sold_date'].dt.to_period('M')).size()\n",
        "print(f\"Months covered: {len(monthly_counts)}\")\n",
        "print(f\"Avg sales/month: {monthly_counts.mean():.0f}\")\n",
        "print(f\"Min sales/month: {monthly_counts.min()}\")\n",
        "print(f\"Max sales/month: {monthly_counts.max()}\")\n",
        "\n",
        "# Yearly summary\n",
        "yearly = training_df.groupby(training_df['sold_date'].dt.year).agg({\n",
        "    'price': ['count', 'median', 'mean']\n",
        "}).round(0)\n",
        "yearly.columns = ['count', 'median_price', 'mean_price']\n",
        "print(f\"\\nYearly Summary:\")\n",
        "print(yearly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Visualize Cleaning Results\n",
        "# =============================================================================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Price distribution\n",
        "axes[0, 0].hist(training_df['price'], bins=100, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Price ($)')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].set_title('Price Distribution (Cleaned Data)')\n",
        "axes[0, 0].axvline(training_df['price'].median(), color='red', linestyle='--', \n",
        "                    label=f\"Median: ${training_df['price'].median():,.0f}\")\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Sales over time\n",
        "monthly_df = training_df.groupby(training_df['sold_date'].dt.to_period('M')).size().reset_index()\n",
        "monthly_df.columns = ['month', 'count']\n",
        "monthly_df['month'] = monthly_df['month'].dt.to_timestamp()\n",
        "axes[0, 1].plot(monthly_df['month'], monthly_df['count'], marker='.', markersize=3)\n",
        "axes[0, 1].set_xlabel('Date')\n",
        "axes[0, 1].set_ylabel('Number of Sales')\n",
        "axes[0, 1].set_title('Sales Volume Over Time (2018-2025)')\n",
        "\n",
        "# Hours distribution\n",
        "valid_hours = training_df[training_df['hours'].notna()]['hours']\n",
        "axes[1, 0].hist(valid_hours, bins=100, edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].set_xlabel('Hours')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "axes[1, 0].set_title(f'Hours Distribution (n={len(valid_hours):,})')\n",
        "axes[1, 0].axvline(valid_hours.median(), color='red', linestyle='--',\n",
        "                    label=f\"Median: {valid_hours.median():,.0f}\")\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Regional distribution\n",
        "region_counts = training_df['region'].value_counts()\n",
        "axes[1, 1].barh(range(len(region_counts)), region_counts.values)\n",
        "axes[1, 1].set_yticks(range(len(region_counts)))\n",
        "axes[1, 1].set_yticklabels(region_counts.index)\n",
        "axes[1, 1].set_xlabel('Number of Sales')\n",
        "axes[1, 1].set_title('Sales by Region')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 7: Inspect Merged Columns\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"MERGED DATASET COLUMNS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTotal columns: {len(training_df.columns)}\")\n",
        "print(\"\\nAll columns:\")\n",
        "for i, col in enumerate(sorted(training_df.columns), 1):\n",
        "    dtype = training_df[col].dtype\n",
        "    non_null = training_df[col].notna().sum()\n",
        "    pct = non_null / len(training_df) * 100\n",
        "    print(f\"  {i:2}. {col:30} {str(dtype):15} {pct:5.1f}% coverage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 8: Sample Data Inspection\n",
        "# =============================================================================\n",
        "print(\"\\n--- Sample Records ---\")\n",
        "print(\"\\nFirst 5 records (key columns):\")\n",
        "key_cols = ['sold_date', 'price', 'year', 'hours', 'make_key', 'region', \n",
        "            'barometer', 'diesel_price', 'el_nino_phase']\n",
        "display_cols = [c for c in key_cols if c in training_df.columns]\n",
        "print(training_df[display_cols].head())\n",
        "\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(training_df[['price', 'year', 'hours']].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 9: Save Processed Dataset\n",
        "# =============================================================================\n",
        "output_path = Path('../data/processed/training_data.parquet')\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Saving processed dataset...\")\n",
        "training_df.to_parquet(output_path, index=False)\n",
        "\n",
        "file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
        "print(f\"âœ“ Saved to: {output_path}\")\n",
        "print(f\"  File size: {file_size_mb:.1f} MB\")\n",
        "print(f\"  Records: {len(training_df):,}\")\n",
        "print(f\"  Columns: {len(training_df.columns)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DATA CLEANING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nNext step: Feature engineering (notebook 03)\")\n",
        "print(\"  - Equipment features (age, utilization)\")\n",
        "print(\"  - Temporal features (seasonality)\")\n",
        "print(\"  - Macro features (normalized indicators)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
